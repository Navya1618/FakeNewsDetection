{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 5444, 1: 4795})\n",
      "Counter({0: 4795, 1: 4795})\n",
      "  fraudulent                                        description\n",
      "0          0  Montana Democratic Sen. Jon Tester is the larg...\n",
      "1          0  Obama \"promises more taxes on small business, ...\n",
      "2          1  Newt Gingrich was fined $300,000 for ethics vi...\n",
      "3          0  Since the time of the Civil War, weve made a d...\n",
      "4          1  Newly Elected Republican Senators Sign Pledge ...\n",
      "5          1  Of our 98,000 teachers who are K-12, over 53,0...\n",
      "6          1  The Pentagon made up the since-debunked heroic...\n",
      "7          0  To say a family has to buy a product from a pr...\n",
      "8          0   90 percent of fires in Arizona are human-caused.\n",
      "9          1  Says Steve Jobs was responsible for creating m...\n",
      "     fraudulent                                        description\n",
      "9580          0  Says that hes responsible for Austinincluding ...\n",
      "9581          1   Says Rick Perry wanted to secede from the union.\n",
      "9582          0  Says Ron Johnsons company got government loans...\n",
      "9583          1  I differed with my party on business legislati...\n",
      "9584          1  71% of WIs roads are in poor or mediocre condi...\n",
      "9585          1  Georgias illegal immigration crackdown laws sh...\n",
      "9586          1  California has the sixth largest economy on pl...\n",
      "9587          0  52 jobs have been saved or created in Florida'...\n",
      "9588          1  You cant read a speech by George Washington . ...\n",
      "9589          0  In Texas, a faceless hospital panel can deny l...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "#######################\n",
    "\n",
    "df2 = pd.read_csv(\"liar_dataset/train.tsv\", sep='\\t')\n",
    "df2.columns = ['Name', 'fraudulent', 'description', 'Weight', 'a', 'b', 'c', 'Name', 'Code', 'Age', 'Weight', 'a', 'b', 'c']\n",
    "\n",
    "df2 = df2[['fraudulent', 'description']]\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    if(df2.at[i,'fraudulent'] == 'half-true'):\n",
    "        df2.at[i,'fraudulent'] = 0\n",
    "    elif(df2.at[i,'fraudulent'] == 'barely-true'):\n",
    "        df2.at[i,'fraudulent'] = 0\n",
    "    elif(df2.at[i,'fraudulent'] == 'true'):\n",
    "        df2.at[i,'fraudulent'] = 0    \n",
    "    else:\n",
    "        df2.at[i,'fraudulent'] = 1\n",
    "\n",
    "df = df2\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(df['fraudulent'].values))\n",
    "\n",
    "df_fraudulent= df[df['fraudulent'] == 1] \n",
    "df_normal = df[df['fraudulent'] == 0] \n",
    "df_normal = df_normal.sample(n=len(df_fraudulent))\n",
    "df = df_normal.append(df_fraudulent)\n",
    "df = df.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
    "\n",
    "print(Counter(df['fraudulent'].values))\n",
    "\n",
    "\n",
    "train_data = df.head(10)\n",
    "print(train_data)\n",
    "test_data = df.tail(10)\n",
    "print(test_data)\n",
    "\n",
    "train_data = [{'description': description, 'fraudulent': fraudulent } for description in list(train_data['description']) for fraudulent in list(train_data['fraudulent'])]\n",
    "test_data = [{'description': description, 'fraudulent': fraudulent } for description in list(test_data['description']) for fraudulent in list(test_data['fraudulent'])]\n",
    "\n",
    "train_texts, train_labels = list(zip(*map(lambda d: (d['description'], d['fraudulent']), train_data)))\n",
    "test_texts, test_labels = list(zip(*map(lambda d: (d['description'], d['fraudulent']), test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montana Democratic Sen. Jon Tester is the largest recipient of lobbyist money.\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0]['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'description': 'Montana Democratic Sen. Jon Tester is the largest recipient of lobbyist money.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Montana Democratic Sen. Jon Tester is the largest recipient of lobbyist money.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Montana Democratic Sen. Jon Tester is the largest recipient of lobbyist money.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Montana Democratic Sen. Jon Tester is the largest recipient of lobbyist money.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Montana Democratic Sen. Jon Tester is the largest recipient of lobbyist money.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Montana Democratic Sen. Jon Tester is the largest recipient of lobbyist money.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Montana Democratic Sen. Jon Tester is the largest recipient of lobbyist money.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Montana Democratic Sen. Jon Tester is the largest recipient of lobbyist money.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Montana Democratic Sen. Jon Tester is the largest recipient of lobbyist money.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Montana Democratic Sen. Jon Tester is the largest recipient of lobbyist money.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Obama \"promises more taxes on small business, seniors, your life savings, your family.\"',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Obama \"promises more taxes on small business, seniors, your life savings, your family.\"',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Obama \"promises more taxes on small business, seniors, your life savings, your family.\"',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Obama \"promises more taxes on small business, seniors, your life savings, your family.\"',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Obama \"promises more taxes on small business, seniors, your life savings, your family.\"',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Obama \"promises more taxes on small business, seniors, your life savings, your family.\"',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Obama \"promises more taxes on small business, seniors, your life savings, your family.\"',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Obama \"promises more taxes on small business, seniors, your life savings, your family.\"',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Obama \"promises more taxes on small business, seniors, your life savings, your family.\"',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Obama \"promises more taxes on small business, seniors, your life savings, your family.\"',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Newt Gingrich was fined $300,000 for ethics violations.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Newt Gingrich was fined $300,000 for ethics violations.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Newt Gingrich was fined $300,000 for ethics violations.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Newt Gingrich was fined $300,000 for ethics violations.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Newt Gingrich was fined $300,000 for ethics violations.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Newt Gingrich was fined $300,000 for ethics violations.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Newt Gingrich was fined $300,000 for ethics violations.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Newt Gingrich was fined $300,000 for ethics violations.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Newt Gingrich was fined $300,000 for ethics violations.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Newt Gingrich was fined $300,000 for ethics violations.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Since the time of the Civil War, weve made a distinction in this country between the availability and the ability to access for people who were in the military, versus the rest of us, to vote.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Since the time of the Civil War, weve made a distinction in this country between the availability and the ability to access for people who were in the military, versus the rest of us, to vote.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Since the time of the Civil War, weve made a distinction in this country between the availability and the ability to access for people who were in the military, versus the rest of us, to vote.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Since the time of the Civil War, weve made a distinction in this country between the availability and the ability to access for people who were in the military, versus the rest of us, to vote.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Since the time of the Civil War, weve made a distinction in this country between the availability and the ability to access for people who were in the military, versus the rest of us, to vote.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Since the time of the Civil War, weve made a distinction in this country between the availability and the ability to access for people who were in the military, versus the rest of us, to vote.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Since the time of the Civil War, weve made a distinction in this country between the availability and the ability to access for people who were in the military, versus the rest of us, to vote.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Since the time of the Civil War, weve made a distinction in this country between the availability and the ability to access for people who were in the military, versus the rest of us, to vote.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Since the time of the Civil War, weve made a distinction in this country between the availability and the ability to access for people who were in the military, versus the rest of us, to vote.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Since the time of the Civil War, weve made a distinction in this country between the availability and the ability to access for people who were in the military, versus the rest of us, to vote.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Newly Elected Republican Senators Sign Pledge to Eliminate Food Stamp Program in 2015.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Newly Elected Republican Senators Sign Pledge to Eliminate Food Stamp Program in 2015.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Newly Elected Republican Senators Sign Pledge to Eliminate Food Stamp Program in 2015.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Newly Elected Republican Senators Sign Pledge to Eliminate Food Stamp Program in 2015.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Newly Elected Republican Senators Sign Pledge to Eliminate Food Stamp Program in 2015.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Newly Elected Republican Senators Sign Pledge to Eliminate Food Stamp Program in 2015.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Newly Elected Republican Senators Sign Pledge to Eliminate Food Stamp Program in 2015.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Newly Elected Republican Senators Sign Pledge to Eliminate Food Stamp Program in 2015.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Newly Elected Republican Senators Sign Pledge to Eliminate Food Stamp Program in 2015.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Newly Elected Republican Senators Sign Pledge to Eliminate Food Stamp Program in 2015.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Of our 98,000 teachers who are K-12, over 53,000 of those teachers today are over 50 years old.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Of our 98,000 teachers who are K-12, over 53,000 of those teachers today are over 50 years old.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Of our 98,000 teachers who are K-12, over 53,000 of those teachers today are over 50 years old.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Of our 98,000 teachers who are K-12, over 53,000 of those teachers today are over 50 years old.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Of our 98,000 teachers who are K-12, over 53,000 of those teachers today are over 50 years old.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Of our 98,000 teachers who are K-12, over 53,000 of those teachers today are over 50 years old.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Of our 98,000 teachers who are K-12, over 53,000 of those teachers today are over 50 years old.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Of our 98,000 teachers who are K-12, over 53,000 of those teachers today are over 50 years old.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Of our 98,000 teachers who are K-12, over 53,000 of those teachers today are over 50 years old.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Of our 98,000 teachers who are K-12, over 53,000 of those teachers today are over 50 years old.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'The Pentagon made up the since-debunked heroics of POW Jessica Lynch as she tried to avoid capture in Iraq.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'The Pentagon made up the since-debunked heroics of POW Jessica Lynch as she tried to avoid capture in Iraq.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'The Pentagon made up the since-debunked heroics of POW Jessica Lynch as she tried to avoid capture in Iraq.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'The Pentagon made up the since-debunked heroics of POW Jessica Lynch as she tried to avoid capture in Iraq.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'The Pentagon made up the since-debunked heroics of POW Jessica Lynch as she tried to avoid capture in Iraq.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'The Pentagon made up the since-debunked heroics of POW Jessica Lynch as she tried to avoid capture in Iraq.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'The Pentagon made up the since-debunked heroics of POW Jessica Lynch as she tried to avoid capture in Iraq.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'The Pentagon made up the since-debunked heroics of POW Jessica Lynch as she tried to avoid capture in Iraq.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'The Pentagon made up the since-debunked heroics of POW Jessica Lynch as she tried to avoid capture in Iraq.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'The Pentagon made up the since-debunked heroics of POW Jessica Lynch as she tried to avoid capture in Iraq.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'To say a family has to buy a product from a private company is clearly beyond the delegated powers of the U.S. Constitution.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'To say a family has to buy a product from a private company is clearly beyond the delegated powers of the U.S. Constitution.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'To say a family has to buy a product from a private company is clearly beyond the delegated powers of the U.S. Constitution.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'To say a family has to buy a product from a private company is clearly beyond the delegated powers of the U.S. Constitution.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'To say a family has to buy a product from a private company is clearly beyond the delegated powers of the U.S. Constitution.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'To say a family has to buy a product from a private company is clearly beyond the delegated powers of the U.S. Constitution.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'To say a family has to buy a product from a private company is clearly beyond the delegated powers of the U.S. Constitution.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'To say a family has to buy a product from a private company is clearly beyond the delegated powers of the U.S. Constitution.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'To say a family has to buy a product from a private company is clearly beyond the delegated powers of the U.S. Constitution.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'To say a family has to buy a product from a private company is clearly beyond the delegated powers of the U.S. Constitution.',\n",
       "  'fraudulent': 1},\n",
       " {'description': '90 percent of fires in Arizona are human-caused.',\n",
       "  'fraudulent': 0},\n",
       " {'description': '90 percent of fires in Arizona are human-caused.',\n",
       "  'fraudulent': 0},\n",
       " {'description': '90 percent of fires in Arizona are human-caused.',\n",
       "  'fraudulent': 1},\n",
       " {'description': '90 percent of fires in Arizona are human-caused.',\n",
       "  'fraudulent': 0},\n",
       " {'description': '90 percent of fires in Arizona are human-caused.',\n",
       "  'fraudulent': 1},\n",
       " {'description': '90 percent of fires in Arizona are human-caused.',\n",
       "  'fraudulent': 1},\n",
       " {'description': '90 percent of fires in Arizona are human-caused.',\n",
       "  'fraudulent': 1},\n",
       " {'description': '90 percent of fires in Arizona are human-caused.',\n",
       "  'fraudulent': 0},\n",
       " {'description': '90 percent of fires in Arizona are human-caused.',\n",
       "  'fraudulent': 0},\n",
       " {'description': '90 percent of fires in Arizona are human-caused.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Says Steve Jobs was responsible for creating more jobs than the stimulus bill.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Says Steve Jobs was responsible for creating more jobs than the stimulus bill.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Says Steve Jobs was responsible for creating more jobs than the stimulus bill.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Says Steve Jobs was responsible for creating more jobs than the stimulus bill.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Says Steve Jobs was responsible for creating more jobs than the stimulus bill.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Says Steve Jobs was responsible for creating more jobs than the stimulus bill.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Says Steve Jobs was responsible for creating more jobs than the stimulus bill.',\n",
       "  'fraudulent': 1},\n",
       " {'description': 'Says Steve Jobs was responsible for creating more jobs than the stimulus bill.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Says Steve Jobs was responsible for creating more jobs than the stimulus bill.',\n",
       "  'fraudulent': 0},\n",
       " {'description': 'Says Steve Jobs was responsible for creating more jobs than the stimulus bill.',\n",
       "  'fraudulent': 1}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6415</th>\n",
       "      <td>A tiger tested positive for COVID-19 please st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6416</th>\n",
       "      <td>???Autopsies prove that COVID-19 is??� a blood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6417</th>\n",
       "      <td>_A post claims a COVID-19 vaccine has already ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6418</th>\n",
       "      <td>Aamir Khan Donate 250 Cr. In PM Relief Cares Fund</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6419</th>\n",
       "      <td>It has been 93 days since the last case of COV...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6420 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            description fraudulent\n",
       "0     The CDC currently reports 99031 deaths. In gen...          0\n",
       "1     States reported 1121 deaths a small rise from ...          0\n",
       "2     Politically Correct Woman (Almost) Uses Pandem...          1\n",
       "3     #IndiaFightsCorona: We have 1524 #COVID testin...          0\n",
       "4     Populous states can generate large case counts...          0\n",
       "...                                                 ...        ...\n",
       "6415  A tiger tested positive for COVID-19 please st...          1\n",
       "6416  ???Autopsies prove that COVID-19 is??� a blood...          1\n",
       "6417  _A post claims a COVID-19 vaccine has already ...          1\n",
       "6418  Aamir Khan Donate 250 Cr. In PM Relief Cares Fund          1\n",
       "6419  It has been 93 days since the last case of COV...          0\n",
       "\n",
       "[6420 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/Constraint_Train.csv\")\n",
    "df.rename(columns={'label':'fraudulent', 'tweet':'description'}, inplace=True)\n",
    "df = df[['description', 'fraudulent']]\n",
    "for i in range(len(df)):\n",
    "    if(df.at[i,'fraudulent'] == 'real'):\n",
    "        df.at[i,'fraudulent'] = 0\n",
    "    else:\n",
    "        df.at[i,'fraudulent'] = 1\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0/100.0 loss: 0.5626635551452637 \n",
      "Epoch:  1\n",
      "1/100.0 loss: 0.670863538980484 \n",
      "Epoch:  1\n",
      "2/100.0 loss: 0.6657837430636088 \n",
      "Epoch:  1\n",
      "3/100.0 loss: 0.6658264994621277 \n",
      "Epoch:  1\n",
      "4/100.0 loss: 0.6882667422294617 \n",
      "Epoch:  1\n",
      "5/100.0 loss: 0.7081780433654785 \n",
      "Epoch:  1\n",
      "6/100.0 loss: 0.7193942240306309 \n",
      "Epoch:  1\n",
      "7/100.0 loss: 0.708170585334301 \n",
      "Epoch:  1\n",
      "8/100.0 loss: 0.6965248783429464 \n",
      "Epoch:  1\n",
      "9/100.0 loss: 0.6873061180114746 \n",
      "Epoch:  1\n",
      "10/100.0 loss: 0.6918655308810148 \n",
      "Epoch:  1\n",
      "11/100.0 loss: 0.6937223970890045 \n",
      "Epoch:  1\n",
      "12/100.0 loss: 0.6818903317818275 \n",
      "Epoch:  1\n",
      "13/100.0 loss: 0.6793080057416644 \n",
      "Epoch:  1\n",
      "14/100.0 loss: 0.6833211501439412 \n",
      "Epoch:  1\n",
      "15/100.0 loss: 0.6861630789935589 \n",
      "Epoch:  1\n",
      "16/100.0 loss: 0.6912383507279789 \n",
      "Epoch:  1\n",
      "17/100.0 loss: 0.708060277832879 \n",
      "Epoch:  1\n",
      "18/100.0 loss: 0.7081224636027688 \n",
      "Epoch:  1\n",
      "19/100.0 loss: 0.7038881778717041 \n",
      "Epoch:  1\n",
      "20/100.0 loss: 0.6946225109554472 \n",
      "Epoch:  1\n",
      "21/100.0 loss: 0.7021523984995756 \n",
      "Epoch:  1\n",
      "22/100.0 loss: 0.7035623503767926 \n",
      "Epoch:  1\n",
      "23/100.0 loss: 0.7083617399136225 \n",
      "Epoch:  1\n",
      "24/100.0 loss: 0.7044456195831299 \n",
      "Epoch:  1\n",
      "25/100.0 loss: 0.7020923151419713 \n",
      "Epoch:  1\n",
      "26/100.0 loss: 0.7075255579418607 \n",
      "Epoch:  1\n",
      "27/100.0 loss: 0.7124100008181163 \n",
      "Epoch:  1\n",
      "28/100.0 loss: 0.7064210813620995 \n",
      "Epoch:  1\n",
      "29/100.0 loss: 0.7096343358357747 \n",
      "Epoch:  1\n",
      "30/100.0 loss: 0.7035975879238497 \n",
      "Epoch:  1\n",
      "31/100.0 loss: 0.70802160538733 \n",
      "Epoch:  1\n",
      "32/100.0 loss: 0.7123700867999684 \n",
      "Epoch:  1\n",
      "33/100.0 loss: 0.705942377448082 \n",
      "Epoch:  1\n",
      "34/100.0 loss: 0.7040252728121621 \n",
      "Epoch:  1\n",
      "35/100.0 loss: 0.701519761648443 \n",
      "Epoch:  1\n",
      "36/100.0 loss: 0.700164682961799 \n",
      "Epoch:  1\n",
      "37/100.0 loss: 0.6970993131399155 \n",
      "Epoch:  1\n",
      "38/100.0 loss: 0.7030813992023468 \n",
      "Epoch:  1\n",
      "39/100.0 loss: 0.700059000402689 \n",
      "Epoch:  1\n",
      "40/100.0 loss: 0.7022035768846187 \n",
      "Epoch:  1\n",
      "41/100.0 loss: 0.6995805182627269 \n",
      "Epoch:  1\n",
      "42/100.0 loss: 0.6946222851442736 \n",
      "Epoch:  1\n",
      "43/100.0 loss: 0.6963651193813845 \n",
      "Epoch:  1\n",
      "44/100.0 loss: 0.6978498842981127 \n",
      "Epoch:  1\n",
      "45/100.0 loss: 0.700255978366603 \n",
      "Epoch:  1\n",
      "46/100.0 loss: 0.696394109979589 \n",
      "Epoch:  1\n",
      "47/100.0 loss: 0.6976706857482592 \n",
      "Epoch:  1\n",
      "48/100.0 loss: 0.6977522993574337 \n",
      "Epoch:  1\n",
      "49/100.0 loss: 0.695908076763153 \n",
      "Epoch:  1\n",
      "50/100.0 loss: 0.6923375515376821 \n",
      "Epoch:  1\n",
      "51/100.0 loss: 0.6913556250242087 \n",
      "Epoch:  1\n",
      "52/100.0 loss: 0.6935112004010182 \n",
      "Epoch:  1\n",
      "53/100.0 loss: 0.6946080062124464 \n",
      "Epoch:  1\n",
      "54/100.0 loss: 0.6904741184277968 \n",
      "Epoch:  1\n",
      "55/100.0 loss: 0.6864421266530242 \n",
      "Epoch:  1\n",
      "56/100.0 loss: 0.6832520412771326 \n",
      "Epoch:  1\n",
      "57/100.0 loss: 0.6818095759071153 \n",
      "Epoch:  1\n",
      "58/100.0 loss: 0.6848392552238399 \n",
      "Epoch:  1\n",
      "59/100.0 loss: 0.6865441386898359 \n",
      "Epoch:  1\n",
      "60/100.0 loss: 0.6868213380946487 \n",
      "Epoch:  1\n",
      "61/100.0 loss: 0.6883034230239929 \n",
      "Epoch:  1\n",
      "62/100.0 loss: 0.6890973831926074 \n",
      "Epoch:  1\n",
      "63/100.0 loss: 0.6905166148208082 \n",
      "Epoch:  1\n",
      "64/100.0 loss: 0.6927619324280665 \n",
      "Epoch:  1\n",
      "65/100.0 loss: 0.6932403720689543 \n",
      "Epoch:  1\n",
      "66/100.0 loss: 0.6945339997312916 \n",
      "Epoch:  1\n",
      "67/100.0 loss: 0.6936635195332415 \n",
      "Epoch:  1\n",
      "68/100.0 loss: 0.692676869423493 \n",
      "Epoch:  1\n",
      "69/100.0 loss: 0.6945701211690902 \n",
      "Epoch:  1\n",
      "70/100.0 loss: 0.6942814701879528 \n",
      "Epoch:  1\n",
      "71/100.0 loss: 0.6941170117093457 \n",
      "Epoch:  1\n",
      "72/100.0 loss: 0.6969563450715314 \n",
      "Epoch:  1\n",
      "73/100.0 loss: 0.6966487269949269 \n",
      "Epoch:  1\n",
      "74/100.0 loss: 0.699257740577062 \n",
      "Epoch:  1\n",
      "75/100.0 loss: 0.6987577194446012 \n",
      "Epoch:  1\n",
      "76/100.0 loss: 0.7012469687245109 \n",
      "Epoch:  1\n",
      "77/100.0 loss: 0.6996273264670984 \n",
      "Epoch:  1\n",
      "78/100.0 loss: 0.6993196927293946 \n",
      "Epoch:  1\n",
      "79/100.0 loss: 0.6989842768758535 \n",
      "Epoch:  1\n",
      "80/100.0 loss: 0.6992325396449478 \n",
      "Epoch:  1\n",
      "81/100.0 loss: 0.69806067427484 \n",
      "Epoch:  1\n",
      "82/100.0 loss: 0.699820566249181 \n",
      "Epoch:  1\n",
      "83/100.0 loss: 0.6990312114357948 \n",
      "Epoch:  1\n",
      "84/100.0 loss: 0.700051867611268 \n",
      "Epoch:  1\n",
      "85/100.0 loss: 0.7012149729700976 \n",
      "Epoch:  1\n",
      "86/100.0 loss: 0.7034169041562355 \n",
      "Epoch:  1\n",
      "87/100.0 loss: 0.7044371850788593 \n",
      "Epoch:  1\n",
      "88/100.0 loss: 0.7052168601684357 \n",
      "Epoch:  1\n",
      "89/100.0 loss: 0.7045511355002722 \n",
      "Epoch:  1\n",
      "90/100.0 loss: 0.7035148644840324 \n",
      "Epoch:  1\n",
      "91/100.0 loss: 0.7041735522772955 \n",
      "Epoch:  1\n",
      "92/100.0 loss: 0.7040830385941331 \n",
      "Epoch:  1\n",
      "93/100.0 loss: 0.7055071677933348 \n",
      "Epoch:  1\n",
      "94/100.0 loss: 0.7065394693299344 \n",
      "Epoch:  1\n",
      "95/100.0 loss: 0.7058543727422754 \n",
      "Epoch:  1\n",
      "96/100.0 loss: 0.7067982631245839 \n",
      "Epoch:  1\n",
      "97/100.0 loss: 0.7076437877757209 \n",
      "Epoch:  1\n",
      "98/100.0 loss: 0.7088840745314203 \n",
      "Epoch:  1\n",
      "99/100.0 loss: 0.7095528075098991 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.40      0.40      0.40        40\n",
      "        True       0.60      0.60      0.60        60\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.50      0.50      0.50       100\n",
      "weighted avg       0.52      0.52      0.52       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))\n",
    "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))\n",
    "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
    "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\n",
    "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "\n",
    "train_y = np.array(train_labels) == 1\n",
    "test_y = np.array(test_labels) == 1\n",
    "\n",
    "\n",
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba\n",
    "    \n",
    "    \n",
    "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
    "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
    "train_masks_tensor = torch.tensor(train_masks)\n",
    "test_masks_tensor = torch.tensor(test_masks)\n",
    "\n",
    "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
    "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
    "train_sampler =  torch.utils.data.RandomSampler(train_dataset)\n",
    "train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
    "test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "EPOCHS = 1\n",
    "bert_clf = BertBinaryClassifier()\n",
    "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
    "for epoch_num in range(EPOCHS):\n",
    "    bert_clf.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        probas = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch: ', epoch_num + 1)\n",
    "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
    "        \n",
    "bert_clf.eval()\n",
    "bert_predicted = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(test_dataloader):\n",
    "\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        loss = loss_func(logits, labels)\n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "        \n",
    "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
    "        all_logits += list(numpy_logits[:, 0])\n",
    "        \n",
    "print(classification_report(test_y, bert_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
