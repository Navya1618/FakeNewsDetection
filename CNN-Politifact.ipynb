{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "molecular-security",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in c:\\users\\navyasree\\anaconda3\\envs\\fakenews\\lib\\site-packages (0.14.9)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in c:\\users\\navyasree\\anaconda3\\envs\\fakenews\\lib\\site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: py-params>=0.9.6 in c:\\users\\navyasree\\anaconda3\\envs\\fakenews\\lib\\site-packages (from bert-for-tf2) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\navyasree\\anaconda3\\envs\\fakenews\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.59.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\navyasree\\anaconda3\\envs\\fakenews\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\navyasree\\anaconda3\\envs\\fakenews\\lib\\site-packages (0.1.95)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defined-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "included-gauge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bibliographic-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cross-nerve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12791, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all = pd.read_csv(\"all.tsv\", delimiter = \"\\t\")\n",
    "train_all.isnull().values.any()\n",
    "train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "usual-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cellular-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crude-working",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12791\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "labels = []\n",
    "\n",
    "with open(\"all.tsv\", 'r', encoding = 'utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        labels.append(row[2])\n",
    "        if(len(row)>13):\n",
    "            article = row[2]+row[3]+row[4]+row[5]+row[7]+row[13]\n",
    "        else:\n",
    "            article = row[1]\n",
    "        articles.append(preprocess_text(article))\n",
    "\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "first-onion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['false' 'halfTrue' 'mostlyTrue' 'true' 'barelyTrue' 'pantsFire']\n"
     ]
    }
   ],
   "source": [
    "print(train_all.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "domestic-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_articles(txt):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "international-volunteer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12791\n"
     ]
    }
   ],
   "source": [
    "tokenized_articles = [tokenize_articles(article) for article in articles]\n",
    "print(len(tokenized_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sapphire-hours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y = train_all['label']\n",
    "label_val = ['true', 'halfTrue', 'mostlyTrue', 'barelyTrue', 'false', 'pantsFire']\n",
    "#label_val = ['real', 'fake']\n",
    "label_token = [0, 0, 0, 1, 1, 1]\n",
    "y = np.array(list(map(lambda x: label_token[label_val.index(x)], y)))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conceptual-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_with_len = [[tokenized_articles[i], y[i], len(tokenized_articles[i])]\n",
    "                 for i in range(0, len(articles))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "virgin-aerospace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6270], 1, 1]\n"
     ]
    }
   ],
   "source": [
    "input_with_len.sort(key=lambda x: x[2])\n",
    "print(input_with_len[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "horizontal-booth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12791\n",
      "([6270], 1)\n"
     ]
    }
   ],
   "source": [
    "sorted_articles_labels = [(article_lab[0], article_lab[1]) for article_lab in input_with_len]\n",
    "print(len(sorted_articles_labels))\n",
    "print(sorted_articles_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "experimental-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_articles_labels, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "useful-peoples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 16), dtype=int32, numpy=\n",
       " array([[ 6270,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 6270,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 2006,  8639,  2510,  5558,  7295, 19186,  2226,  5205,  2890,\n",
       "         14289, 16558,  5555,  2078,     0,     0,     0],\n",
       "        [10210,  2102, 19615,  2038,  4013,  3601,  2501, 11324, 21197,\n",
       "          6644, 10643,  8540,  2063,     0,     0,     0],\n",
       "        [ 2006, 12195, 15827,  2943, 20709,  3600,  8112, 28994,  5178,\n",
       "          3372,  3207,  5302, 23185,     0,     0,     0],\n",
       "        [ 8112, 25316, 22555,  5160,  2005,  2012,  3425,  3423,  3314,\n",
       "         20740,  6856,  8540,  5243,  2739,  2713,     0],\n",
       "        [ 3032, 18897,  8112,  5747,  3097,  3343,  2510, 20444,  2078,\n",
       "          9056, 24147, 22084,  1056, 28394,  2102,     0],\n",
       "        [ 2019, 10812,  5653,  2758,  2008, 18520,  7207, 13010, 15523,\n",
       "          4610, 24925,  2078, 10373,  8540,  2063,     0],\n",
       "        [ 1996, 11006,  3367,  2221,  1999,  2637,  4044, 24463,  2389,\n",
       "          2497,  2221,  8540, 20175,  2015,  4037,     0],\n",
       "        [ 1996,  2665,  3016, 15285,  2024,  6102,  3029,  2998,  9363,\n",
       "          4779,  3145,  2229,  8540, 11219,  3720,     0],\n",
       "        [ 2417,  2422,  4950,  5361,  2000,  5326,  2270,  3808,  5193,\n",
       "         12972,  5865,  8540,  5243,  2811,  2713,     0],\n",
       "        [ 7150,  3066,  5552,  1996,  3246,  6566,  2565,  2495, 24085,\n",
       "          4679,  8540, 12870, 20414, 19969,  4748,     0],\n",
       "        [ 2006, 21029,  4540, 27373,  2976,  5166, 24457,  5912,  3207,\n",
       "          5302, 23423,  8189,  2618,  2723,  4494,     0],\n",
       "        [ 2006, 20767,  3864, 11285,  6890,  3995, 23062,  2953,  2890,\n",
       "         14289, 16558,  5555,  2532,  2811,  2713,     0],\n",
       "        [ 2006,  2478,  1996,  6143, 11540,  3914,  2943, 20709,  3600,\n",
       "          8112, 28994,  5178,  3372,  3207,  5302, 23185],\n",
       "        [ 2006, 12195, 15827,  2005,  2943, 24454,  6292,  6830,  2501,\n",
       "         10665,  6654,  2226,  5205,  3207,  5302, 23185],\n",
       "        [ 4126,  9466,  1999,  4279,  2007, 27300,  4126, 12219,  3351,\n",
       "         21759,  2401,  2155,  2473,  8540,  5243,  8312],\n",
       "        [ 2006,  4637,  1996,  6111,  2082,  7450,  2495,  5558,  7295,\n",
       "         19398,  2890, 14289, 16558,  5555,  7229,  8874],\n",
       "        [ 2006,  4171,  7807,  2005,  5661,  2696, 20156, 13238,  9957,\n",
       "         19321, 23846, 18124,  5302, 23185,  2050,  4613],\n",
       "        [ 6174, 11685,  6824,  9114,  7574,  7574, 11285, 11685,  6824,\n",
       "          2890, 14289, 16558,  5555,  2532,  2678,  4748],\n",
       "        [ 1045,  4013,  2166,  2002,  2025, 11324, 21559,  2829,  5963,\n",
       "          2226,  5205,  2890, 14289, 16558,  5555,  2078],\n",
       "        [ 1996,  2808,  4532, 28619,  2078,  2699,  2000,  2031,  7917,\n",
       "          5347,  8308, 24925,  2078, 10373,  8540,  2063],\n",
       "        [ 2006, 12042,  5166,  7450,  2976,  5166, 10665,  6654,  2226,\n",
       "          5205,  3207,  5302, 23185,  2050,  2694,  4357],\n",
       "        [ 8879, 14548,  3695, 29158,  8402, 12163,  6666,  4610, 10354,\n",
       "          2140, 25022, 17175, 22084,  3049,  5653,  2121],\n",
       "        [ 1996,  2277, 13857,  8112,  7460,  2000,  2069, 14456,  2304,\n",
       "          2111,  4676, 24925,  2078, 10373,  8540,  2063],\n",
       "        [25597, 12042,  1996,  2976,  5166,  2976,  5166,  4471,  3698,\n",
       "          3045,  2256,  2925,  8540,  5243,  2678,  4748],\n",
       "        [ 4441,  2001,  2019, 25672, 24894, 14088, 11560,  7521, 20444,\n",
       "          2078, 15317, 22648, 29068, 11921, 13229,  4357],\n",
       "        [ 2345,  2602,  3616,  8398,  2180,  1996,  2759,  3789,  3864,\n",
       "         16558,  8649, 14739,  8540,  7974, 15878,  8466],\n",
       "        [19186, 29158,  2450,  2157,  2000,  5454, 11324,  5302,  3726,\n",
       "         15422,  6633, 10085, 14660,  2078,  5653,  4471],\n",
       "        [ 2006,  6274,  7773,  7773,  5193,  5092,  2497, 23149,  3995,\n",
       "         23062,  2953,  2890, 14289, 16558,  5555,  2078],\n",
       "        [10210,  2102, 19615,  4941,  1996,  3282,  9568,  2130,  8923,\n",
       "         11531,  4409, 21197,  6644, 10643,  8540,  2063],\n",
       "        [ 2006,  2270, 12135,  1997,  2010,  3049,  9615, 20709,  3600,\n",
       "          8112, 28994,  5178,  3372,  3207,  5302, 23185]])>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 0, 1, 0, 1])>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n",
    "next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "scientific-gasoline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "TOTAL_BATCHES = math.ceil(len(sorted_articles_labels) / BATCH_SIZE)\n",
    "print(TOTAL_BATCHES)\n",
    "TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "opposed-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "confident-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "wireless-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "unusual-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"accuracy\"])\n",
    "else:\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "previous-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "360/360 [==============================] - 54s 144ms/step - loss: 0.6680 - accuracy: 0.5880\n",
      "Epoch 2/20\n",
      "360/360 [==============================] - 53s 145ms/step - loss: 0.5643 - accuracy: 0.7084\n",
      "Epoch 3/20\n",
      "360/360 [==============================] - 53s 146ms/step - loss: 0.3350 - accuracy: 0.8606\n",
      "Epoch 4/20\n",
      "360/360 [==============================] - 52s 145ms/step - loss: 0.0874 - accuracy: 0.9720\n",
      "Epoch 5/20\n",
      "360/360 [==============================] - 53s 146ms/step - loss: 0.0534 - accuracy: 0.9813\n",
      "Epoch 6/20\n",
      "360/360 [==============================] - 53s 145ms/step - loss: 0.0416 - accuracy: 0.9862\n",
      "Epoch 7/20\n",
      "360/360 [==============================] - 53s 145ms/step - loss: 0.0183 - accuracy: 0.9938\n",
      "Epoch 8/20\n",
      "360/360 [==============================] - 52s 144ms/step - loss: 0.0172 - accuracy: 0.9945\n",
      "Epoch 9/20\n",
      "360/360 [==============================] - 52s 145ms/step - loss: 0.0300 - accuracy: 0.9902\n",
      "Epoch 10/20\n",
      "360/360 [==============================] - 53s 146ms/step - loss: 0.0256 - accuracy: 0.9916\n",
      "Epoch 11/20\n",
      "360/360 [==============================] - 53s 145ms/step - loss: 0.0089 - accuracy: 0.9968\n",
      "Epoch 12/20\n",
      "360/360 [==============================] - 53s 145ms/step - loss: 8.7459e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "360/360 [==============================] - 54s 149ms/step - loss: 2.5749e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "360/360 [==============================] - 53s 147ms/step - loss: 9.3236e-04 - accuracy: 0.9996\n",
      "Epoch 15/20\n",
      "360/360 [==============================] - 52s 144ms/step - loss: 4.7914e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "360/360 [==============================] - 52s 144ms/step - loss: 0.0051 - accuracy: 0.9985\n",
      "Epoch 17/20\n",
      "360/360 [==============================] - 52s 145ms/step - loss: 0.0223 - accuracy: 0.9931\n",
      "Epoch 18/20\n",
      "360/360 [==============================] - 50s 137ms/step - loss: 0.0465 - accuracy: 0.9841\n",
      "Epoch 19/20\n",
      "360/360 [==============================] - 49s 135ms/step - loss: 0.0159 - accuracy: 0.9955\n",
      "Epoch 20/20\n",
      "360/360 [==============================] - 49s 136ms/step - loss: 0.0063 - accuracy: 0.9977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x204857d0ba8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "appreciated-african",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 21ms/step - loss: 2.5920 - accuracy: 0.5859\n",
      "[2.592012882232666, 0.5859375]\n"
     ]
    }
   ],
   "source": [
    "results = text_model.evaluate(test_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "residential-inspector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  6104400   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              multiple                  40100     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            multiple                  60100     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            multiple                  80100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  77056     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  257       \n",
      "=================================================================\n",
      "Total params: 6,362,013\n",
      "Trainable params: 6,362,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "willing-billy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'halfTrue': 2627, 'false': 2507, 'mostlyTrue': 2454, 'barelyTrue': 2103, 'true': 2053, 'pantsFire': 1047})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(train_all['label'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-concentration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
