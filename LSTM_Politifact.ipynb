{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/pranav/anaconda3/envs/fake_news/lib/python3.6/site-packages (3.5)\r\n",
      "Requirement already satisfied: regex in /home/pranav/anaconda3/envs/fake_news/lib/python3.6/site-packages (from nltk) (2020.6.8)\r\n",
      "Requirement already satisfied: joblib in /home/pranav/anaconda3/envs/fake_news/lib/python3.6/site-packages (from nltk) (0.16.0)\r\n",
      "Requirement already satisfied: tqdm in /home/pranav/anaconda3/envs/fake_news/lib/python3.6/site-packages (from nltk) (4.47.0)\r\n",
      "Requirement already satisfied: click in /home/pranav/anaconda3/envs/fake_news/lib/python3.6/site-packages (from nltk) (7.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "from torch.utils.data import TensorDataset, random_split,DataLoader, RandomSampler, SequentialSampler\n",
    "import gc\n",
    "import re,nltk\n",
    "from collections import Counter\n",
    "#nltk.download('punkt')\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "torch.manual_seed(1)\n",
    "print(\"Imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "{'done': 1, 'well': 1, 'good': 1, 'work': 2, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
      "Requirement already satisfied: click in c:\\python36\\lib\\site-packages (from nltk) (7.1.1)\n",
      "Requirement already satisfied: joblib in c:\\python36\\lib\\site-packages (from nltk) (0.15.1)\n",
      "Requirement already satisfied: regex in c:\\python36\\lib\\site-packages (from nltk) (2020.5.14)\n",
      "Requirement already satisfied: tqdm in c:\\python36\\lib\\site-packages (from nltk) (4.46.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk: started\n",
      "  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Black Duck\\AppData\\Local\\pip\\Cache\\wheels\\ae\\8c\\3f\\b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  The script nltk.exe is installed in 'c:\\python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "You are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set processing device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'liar_dataset/train.tsv'\n",
    "test_path = 'liar_dataset/test.tsv'\n",
    "val_path = 'liar_dataset/valid.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext.data' has no attribute 'Field'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d79e44260e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTEXT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spacy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minclude_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mLABEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabelField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Field'"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(a):\n",
    "    a_cat = [0]*len(a)\n",
    "    for i in range(len(a)):\n",
    "        if a[i]=='true':\n",
    "            a_cat[i] = 1\n",
    "        elif a[i]=='mostly-true':\n",
    "            a_cat[i] = 1\n",
    "        elif a[i]=='half-true':\n",
    "            a_cat[i] = 1\n",
    "        elif a[i]=='barely-true':\n",
    "            a_cat[i] = 0\n",
    "        elif a[i]=='false':\n",
    "            a_cat[i] = 0\n",
    "        elif a[i]=='pants-fire':\n",
    "            a_cat[i] = 0\n",
    "        else:\n",
    "            print('Incorrect label')\n",
    "    return a_cat\n",
    "\n",
    "def build_dataset_train(statements,labels,length):\n",
    "    count=Counter()\n",
    "    # Clean the sentences\n",
    "    for i in range(len(statements)):\n",
    "        statements[i]=re.sub('\\d','0',statements[i])\n",
    "    #Count the appearance of words. Remove word if appeared only once in set\n",
    "    for i,sentence in enumerate(statements):\n",
    "        statements[i]=[]\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            word=word.lower()\n",
    "            count.update([word])\n",
    "            statements[i].append(word)\n",
    "        if i%1000==0:\n",
    "            print(i,\" sentences done\")\n",
    "    count={key:value for key,value in count.items() if value>1}\n",
    "    \n",
    "    count=sorted(count,reverse=True, key=lambda key:count[key])\n",
    "    count+=['_padding','_unknown']\n",
    "\n",
    "    word_to_idx={word:index for index,word in enumerate(count)}\n",
    "    idx_to_word={index:word for index,word in enumerate(count)}\n",
    "    \n",
    "    #Tokenize sentences\n",
    "    for i,sentence in enumerate(statements):\n",
    "        statements[i]=[word_to_idx[word] if word in count else 0 for word in sentence ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_statements=padding(statements,length)\n",
    "    train_label=np.array(labels)\n",
    "    \n",
    "    print(\"Done build...train\")\n",
    "    return train_statements,train_label,word_to_idx,idx_to_word\n",
    "\n",
    "def build_dataset_test(statements,labels,length,word_to_idx):\n",
    "    # Clean the sentences\n",
    "    for i in range(len(statements)):\n",
    "        statements[i]=re.sub('\\d','0',statements[i])\n",
    "        statements[i]=[word_to_idx[word.lower()] if word.lower() in word_to_idx else 0 for word in nltk.word_tokenize(statements[i]) ]\n",
    "\n",
    "    test_statements=padding(statements,length)\n",
    "    test_label=np.array(labels)\n",
    "    print(\"Done build...test\")\n",
    "    return test_statements,test_label\n",
    "\n",
    "\n",
    "def padding(statements, length):\n",
    "    array=np.zeros((len(statements),length),dtype=int)\n",
    "    for i,indexes in enumerate(statements):\n",
    "        if len(indexes)!=0:\n",
    "            array[i,-len(indexes):]=np.array(indexes)[:length]\n",
    "    return array\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_liar_dataset():\n",
    "    train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
    "    test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
    "    val_df = pd.read_csv(val_path, sep=\"\\t\", header=None)\n",
    "\n",
    "    train = train_df.values\n",
    "    test = test_df.values\n",
    "    val = val_df.values\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    labels = {'train':train[:,1], 'test':test[:,1], 'val':val[:,1]}\n",
    "    statements = {'train':train[:,2], 'test':test[:,2], 'val':val[:,2]}\n",
    "    subjects = {'train':train[:,3], 'test':test[:,3], 'val':val[:,3]}\n",
    "    speaker = {'train':train[:,4], 'test':test[:,4], 'val':val[:,4]}\n",
    "    job = {'train':train[:,5], 'test':test[:,5], 'val':val[:,5]}\n",
    "    state = {'train':train[:,6], 'test':test[:,6], 'val':val[:,6]}\n",
    "    affiliation = {'train':train[:,7], 'test':test[:,7], 'val':val[:,7]}\n",
    "    \n",
    "    length=20\n",
    "    labels_onehot = {'train':to_onehot(labels['train']), 'test':to_onehot(labels['test']), 'val':to_onehot(labels['val'])}\n",
    "    print(\"Building training set\")\n",
    "    train_dataset,train_label,word_to_idx,idx_to_word = build_dataset_train(statements['train'],labels_onehot['train'],length)\n",
    "    print(\"Building valid set\")\n",
    "    val_dataset,val_label = build_dataset_test(statements['val'],labels_onehot['val'],length,word_to_idx)\n",
    "    print(\"Building testing set\")\n",
    "    test_dataset,test_label = build_dataset_test(statements['test'],labels_onehot['test'],length,word_to_idx)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset,train_label,val_label,test_label,word_to_idx,idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pranav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set\n",
      "0  sentences done\n",
      "1000  sentences done\n",
      "2000  sentences done\n",
      "3000  sentences done\n",
      "4000  sentences done\n",
      "5000  sentences done\n",
      "6000  sentences done\n",
      "7000  sentences done\n",
      "8000  sentences done\n",
      "9000  sentences done\n",
      "10000  sentences done\n",
      "Done build...train\n",
      "Building valid set\n",
      "Done build...test\n",
      "Building testing set\n",
      "Done build...test\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "train_dataset, val_dataset, test_dataset,train_label,val_label,test_label,word_to_idx,idx_to_word = get_liar_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning into Tensor Dataset\n",
    "batch_size=20\n",
    "train_data=TensorDataset(torch.from_numpy(train_dataset),torch.from_numpy(train_label))\n",
    "val_data=TensorDataset(torch.from_numpy(val_dataset),torch.from_numpy(val_label))\n",
    "test_data=TensorDataset(torch.from_numpy(test_dataset),torch.from_numpy(test_label))\n",
    "train_loader=DataLoader(train_data,shuffle=False,batch_size=batch_size)\n",
    "val_loader=DataLoader(val_data,shuffle=False,batch_size=batch_size)\n",
    "test_loader=DataLoader(test_data,shuffle=False,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'says',\n",
       " 'john',\n",
       " 'mccain',\n",
       " 'has',\n",
       " 'done',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'help',\n",
       " 'the',\n",
       " 'vets',\n",
       " '.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Class\n",
    "class FakeNet(nn.Module):\n",
    "    def __init__(self,vocab_len):\n",
    "        super(FakeNet,self).__init__()\n",
    "        self.outputs=1 #output size [1 and 0] \n",
    "        self.num_layers=3\n",
    "        self.drop_rate=0.5\n",
    "        self.embed_dim=400\n",
    "        self.embed=nn.Embedding(vocab_len,self.embed_dim) \n",
    "        self.hidden_dim=512\n",
    "        self.dropout=nn.Dropout(self.drop_rate)\n",
    "        self.fc=nn.Linear(self.hidden_dim,self.outputs)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.lstm=nn.LSTM(self.embed_dim,self.hidden_dim,self.num_layers,dropout=self.drop_rate,batch_first=True)\n",
    "    \n",
    "    \n",
    "    def hidden_initialize(self,batch_size):\n",
    "            weights=next(self.parameters()).data\n",
    "            hidden=(weights.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(device),weights.new(self.num_layers,batch_size,self.hidden_dim).zero_().to(device))\n",
    "            return hidden\n",
    "    \n",
    "    \n",
    "    #forward propagation\n",
    "    def forward(self,cell,hiddens):\n",
    "        batch_size=cell.size(0)\n",
    "        cell=cell.long()\n",
    "        embeddings=self.embed(cell)\n",
    "        lstm_output,hiddens=self.lstm(embeddings,hiddens)\n",
    "        lstm_output=lstm_output.contiguous().view(-1,self.hidden_dim)\n",
    "        \n",
    "        out_of_cell=self.dropout(lstm_output)\n",
    "        out_of_cell=self.fc(out_of_cell)\n",
    "        out_of_cell=self.sigmoid(out_of_cell)\n",
    "        out_of_cell=out_of_cell.view(batch_size,-1)\n",
    "        out_of_cell=out_of_cell[:,-1]\n",
    "        return out_of_cell,hiddens\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6823\n",
      "6824\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criteria=nn.BCELoss()\n",
    "print(len(word_to_idx))\n",
    "vocab_len=len(word_to_idx)+1\n",
    "print(vocab_len)\n",
    "lr=0.005\n",
    "#initialize model\n",
    "model=FakeNet(vocab_len)\n",
    "model.to(device) #set gpu to model\n",
    "\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(i,epoch,count,valid_losses,curr_loss):\n",
    "    print(\"Now epoch \", i+1, \" out of \", epoch)\n",
    "    print(\"Count:  \",count)\n",
    "    print(\"Valid loss: \",valid_losses)\n",
    "    print(\"Training loss: \",curr_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  1  out of  5\n",
      "Count:   100\n",
      "Valid loss:  tensor(0.7592, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7679475611075759\n",
      "Lower valid loss found, saving model state\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  1  out of  5\n",
      "Count:   200\n",
      "Valid loss:  tensor(0.8867, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7227045018225908\n",
      "Lower valid loss found, saving model state\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  1  out of  5\n",
      "Count:   300\n",
      "Valid loss:  tensor(0.6836, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.6969650303944945\n",
      "Lower valid loss found, saving model state\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  1  out of  5\n",
      "Count:   400\n",
      "Valid loss:  tensor(0.7102, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7133432039991021\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  1  out of  5\n",
      "Count:   500\n",
      "Valid loss:  tensor(0.6307, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.6951235197484493\n",
      "Lower valid loss found, saving model state\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  2  out of  5\n",
      "Count:   600\n",
      "Valid loss:  tensor(0.7022, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.6943556424230337\n",
      "Lower valid loss found, saving model state\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  2  out of  5\n",
      "Count:   700\n",
      "Valid loss:  tensor(0.7454, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7014955412596464\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  2  out of  5\n",
      "Count:   800\n",
      "Valid loss:  tensor(0.6780, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.724190597422421\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  2  out of  5\n",
      "Count:   900\n",
      "Valid loss:  tensor(0.7877, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.6998289059847593\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  2  out of  5\n",
      "Count:   1000\n",
      "Valid loss:  tensor(0.6628, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7164171226322651\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  3  out of  5\n",
      "Count:   1100\n",
      "Valid loss:  tensor(0.7377, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7297212928533554\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  3  out of  5\n",
      "Count:   1200\n",
      "Valid loss:  tensor(0.7621, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.6966298082843423\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  3  out of  5\n",
      "Count:   1300\n",
      "Valid loss:  tensor(0.7953, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7077397089451551\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  3  out of  5\n",
      "Count:   1400\n",
      "Valid loss:  tensor(0.6750, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7577650034800172\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  3  out of  5\n",
      "Count:   1500\n",
      "Valid loss:  tensor(0.6761, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7112682154402137\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  4  out of  5\n",
      "Count:   1600\n",
      "Valid loss:  tensor(0.7686, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7192102754488587\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  4  out of  5\n",
      "Count:   1700\n",
      "Valid loss:  tensor(0.7071, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7146995970979333\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  4  out of  5\n",
      "Count:   1800\n",
      "Valid loss:  tensor(0.5852, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7215460408478975\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  4  out of  5\n",
      "Count:   1900\n",
      "Valid loss:  tensor(0.7173, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.6982983397319913\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  4  out of  5\n",
      "Count:   2000\n",
      "Valid loss:  tensor(0.8191, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.714476271532476\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  5  out of  5\n",
      "Count:   2100\n",
      "Valid loss:  tensor(0.7036, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7188725350424647\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  5  out of  5\n",
      "Count:   2200\n",
      "Valid loss:  tensor(0.6628, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7008341997861862\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  5  out of  5\n",
      "Count:   2300\n",
      "Valid loss:  tensor(0.6821, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.6886194562539458\n",
      "Lower valid loss found, saving model state\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  5  out of  5\n",
      "Count:   2400\n",
      "Valid loss:  tensor(0.7897, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.7101258058100939\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[20, 20]\n",
      "[4, 20]\n",
      "Now epoch  5  out of  5\n",
      "Count:   2500\n",
      "Valid loss:  tensor(0.6764, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Training loss:  0.695500560104847\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "min_valid_loss=99**3\n",
    "count=0\n",
    "clip=5\n",
    "num_epoch = 5\n",
    "model.train()\n",
    "for i in range(num_epoch):\n",
    "    model_hidden = model.hidden_initialize(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "        \n",
    "        count += 1\n",
    "        model_hidden = tuple([ele.data for ele in model_hidden])\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        model.zero_grad()\n",
    "        res = model(inputs, model_hidden)\n",
    "        output = res[0]\n",
    "        model_hidden=res[1]\n",
    "        curr_loss = criteria(output.squeeze(), labels.float())\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        curr_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if count%100 == 0:\n",
    "            valid_h = model.hidden_initialize(batch_size)\n",
    "            valid_losses = []\n",
    "            model.eval()\n",
    "            for input, labeling in val_loader:\n",
    "                print(list(input.size()))\n",
    "                valid_h = tuple([each.data for each in valid_h])\n",
    "                input, labeling = input.to(device), labeling.to(device)\n",
    "                if(list(input.size())==[20,20]):\n",
    "                  out_of_cell, valid_h = model(input, valid_h)\n",
    "                  valid_loss = criteria(out_of_cell.squeeze(), labeling.float())\n",
    "                  valid_losses.append(valid_loss.item())\n",
    "    \n",
    "            model.train()\n",
    "            valid_loss_mean=np.mean(valid_losses)\n",
    "            print_results(i,num_epoch,count,curr_loss,valid_loss_mean)\n",
    "            \n",
    "            if valid_loss_mean <= min_valid_loss:\n",
    "                \n",
    "                print(\"Lower valid loss found, saving model state\")\n",
    "                #valid_loss_min =valid_loss_mean\n",
    "                min_valid_loss = valid_loss_mean\n",
    "                torch.save(model.state_dict(), './bestmodelyet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20])\n",
      "tensor([0.5521, 0.5521, 0.5521, 0.5521, 0.4394, 0.4380, 0.5503, 0.5506, 0.5520,\n",
      "        0.5483, 0.5522, 0.5520, 0.5536, 0.5495, 0.5857, 0.4251, 0.5521, 0.5526,\n",
      "        0.5522, 0.5521], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5522, 0.5522, 0.5521, 0.4395, 0.4379, 0.5494, 0.5496, 0.5524,\n",
      "        0.5470, 0.5523, 0.5521, 0.5525, 0.5494, 0.5524, 0.4550, 0.5522, 0.5534,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5524, 0.5523, 0.5523, 0.5522, 0.4395, 0.4378, 0.5497, 0.5509, 0.5526,\n",
      "        0.5472, 0.5524, 0.5522, 0.5523, 0.5507, 0.5523, 0.4537, 0.5522, 0.5538,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5523, 0.5522, 0.5523, 0.5523, 0.4395, 0.4378, 0.5496, 0.5506, 0.5525,\n",
      "        0.5472, 0.5525, 0.5521, 0.5524, 0.5507, 0.5524, 0.4534, 0.5523, 0.5516,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5523, 0.5523, 0.5523, 0.5522, 0.4395, 0.4377, 0.5496, 0.5497, 0.5526,\n",
      "        0.5463, 0.5524, 0.5522, 0.5524, 0.5505, 0.5524, 0.4531, 0.5523, 0.5517,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5523, 0.5523, 0.5524, 0.5523, 0.4394, 0.4376, 0.5496, 0.5503, 0.5525,\n",
      "        0.5461, 0.5525, 0.5522, 0.5524, 0.5507, 0.5524, 0.4529, 0.5523, 0.5517,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5523, 0.5523, 0.5523, 0.5523, 0.4394, 0.4375, 0.5495, 0.5508, 0.5525,\n",
      "        0.5438, 0.5525, 0.5523, 0.5524, 0.5503, 0.5524, 0.4527, 0.5523, 0.5516,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5523, 0.5523, 0.5524, 0.5523, 0.4394, 0.4375, 0.5495, 0.5506, 0.5525,\n",
      "        0.5437, 0.5525, 0.5524, 0.5524, 0.5505, 0.5524, 0.4524, 0.5526, 0.5516,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5524, 0.5523, 0.4394, 0.4374, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5525, 0.5523, 0.5524, 0.5503, 0.5524, 0.4522, 0.5525, 0.5515,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5525, 0.5523, 0.4394, 0.4374, 0.5495, 0.5499, 0.5525,\n",
      "        0.5438, 0.5525, 0.5523, 0.5524, 0.5505, 0.5523, 0.4521, 0.5525, 0.5516,\n",
      "        0.5525, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5525, 0.5525, 0.5523, 0.4393, 0.4373, 0.5495, 0.5495, 0.5526,\n",
      "        0.5440, 0.5525, 0.5524, 0.5524, 0.5505, 0.5524, 0.4519, 0.5525, 0.5515,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5526, 0.5523, 0.4393, 0.4373, 0.5495, 0.5495, 0.5526,\n",
      "        0.5440, 0.5525, 0.5523, 0.5524, 0.5505, 0.5524, 0.4517, 0.5526, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5523, 0.4393, 0.4373, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5525, 0.5524, 0.5524, 0.5503, 0.5524, 0.4515, 0.5525, 0.5516,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5524, 0.5526, 0.5523, 0.4393, 0.4372, 0.5495, 0.5495, 0.5525,\n",
      "        0.5439, 0.5525, 0.5523, 0.5526, 0.5505, 0.5524, 0.4514, 0.5525, 0.5516,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5525, 0.5524, 0.4393, 0.4372, 0.5495, 0.5495, 0.5526,\n",
      "        0.5438, 0.5525, 0.5523, 0.5526, 0.5503, 0.5524, 0.4512, 0.5525, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5525, 0.5523, 0.4393, 0.4372, 0.5495, 0.5496, 0.5526,\n",
      "        0.5440, 0.5533, 0.5524, 0.5526, 0.5503, 0.5524, 0.4510, 0.5525, 0.5516,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5526, 0.5524, 0.4393, 0.4372, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5533, 0.5523, 0.5526, 0.5505, 0.5524, 0.4509, 0.5526, 0.5516,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5524, 0.4393, 0.4372, 0.5495, 0.5496, 0.5525,\n",
      "        0.5440, 0.5533, 0.5523, 0.5526, 0.5503, 0.5524, 0.4508, 0.5525, 0.5515,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5525, 0.5523, 0.4393, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5440, 0.5525, 0.5524, 0.5526, 0.5503, 0.5524, 0.4506, 0.5526, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5526, 0.5524, 0.4393, 0.4371, 0.5495, 0.5496, 0.5526,\n",
      "        0.5438, 0.5525, 0.5523, 0.5526, 0.5505, 0.5524, 0.4505, 0.5525, 0.5515,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5525, 0.5523, 0.4393, 0.4371, 0.5495, 0.5495, 0.5526,\n",
      "        0.5440, 0.5533, 0.5523, 0.5526, 0.5505, 0.5524, 0.4503, 0.5526, 0.5516,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5524, 0.5526, 0.5524, 0.4393, 0.4371, 0.5495, 0.5496, 0.5526,\n",
      "        0.5440, 0.5533, 0.5525, 0.5526, 0.5503, 0.5524, 0.4502, 0.5526, 0.5515,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5525, 0.5523, 0.4393, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5438, 0.5525, 0.5526, 0.5526, 0.5505, 0.5524, 0.4501, 0.5525, 0.5516,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5526, 0.5526, 0.4393, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5438, 0.5525, 0.5526, 0.5526, 0.5505, 0.5524, 0.4500, 0.5526, 0.5515,\n",
      "        0.5525, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5524, 0.5526, 0.5526, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5527, 0.5525, 0.5526, 0.5494, 0.5524, 0.4499, 0.5525, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5526,\n",
      "        0.5440, 0.5530, 0.5526, 0.5526, 0.5494, 0.5524, 0.4497, 0.5526, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5524, 0.5525, 0.5526, 0.4392, 0.4371, 0.5495, 0.5496, 0.5526,\n",
      "        0.5440, 0.5533, 0.5525, 0.5526, 0.5494, 0.5524, 0.4496, 0.5525, 0.5516,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5525, 0.5526, 0.4392, 0.4371, 0.5495, 0.5495, 0.5526,\n",
      "        0.5438, 0.5525, 0.5525, 0.5526, 0.5494, 0.5524, 0.4495, 0.5525, 0.5516,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5526, 0.5525, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5440, 0.5533, 0.5525, 0.5526, 0.5494, 0.5524, 0.4494, 0.5525, 0.5516,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5526, 0.4392, 0.4371, 0.5495, 0.5495, 0.5526,\n",
      "        0.5440, 0.5533, 0.5525, 0.5526, 0.5494, 0.5524, 0.4493, 0.5525, 0.5516,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5525, 0.5525, 0.5526, 0.5494, 0.5527, 0.4492, 0.5526, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5524, 0.5526, 0.5525, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5440, 0.5525, 0.5526, 0.5526, 0.5494, 0.5526, 0.4491, 0.5525, 0.5515,\n",
      "        0.5525, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5438, 0.5525, 0.5526, 0.5526, 0.5494, 0.5526, 0.4490, 0.5526, 0.5516,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5526, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5438, 0.5525, 0.5525, 0.5526, 0.5494, 0.5524, 0.4489, 0.5525, 0.5516,\n",
      "        0.5525, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5440, 0.5525, 0.5526, 0.5526, 0.5494, 0.5524, 0.4488, 0.5525, 0.5515,\n",
      "        0.5525, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5525, 0.5525, 0.5526, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5525, 0.5526, 0.5526, 0.5494, 0.5524, 0.4488, 0.5525, 0.5516,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5526, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5525, 0.5525, 0.5526, 0.5495, 0.5524, 0.4487, 0.5526, 0.5516,\n",
      "        0.5525, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5525, 0.5525, 0.5526, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5440, 0.5525, 0.5525, 0.5526, 0.5495, 0.5524, 0.4486, 0.5525, 0.5515,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5438, 0.5525, 0.5525, 0.5526, 0.5494, 0.5525, 0.4485, 0.5525, 0.5516,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5525, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5526,\n",
      "        0.5438, 0.5525, 0.5526, 0.5526, 0.5495, 0.5524, 0.4484, 0.5525, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5525, 0.5525, 0.5526, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5438, 0.5525, 0.5526, 0.5526, 0.5495, 0.5524, 0.4484, 0.5525, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5440, 0.5525, 0.5525, 0.5526, 0.5494, 0.5525, 0.4483, 0.5526, 0.5516,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5526, 0.5525, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5440, 0.5525, 0.5525, 0.5526, 0.5494, 0.5526, 0.4482, 0.5525, 0.5515,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5526, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5525, 0.5526, 0.5526, 0.5494, 0.5526, 0.4481, 0.5526, 0.5516,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5526, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5438, 0.5525, 0.5526, 0.5526, 0.5494, 0.5526, 0.4481, 0.5525, 0.5515,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5525, 0.5525, 0.5526, 0.5495, 0.5527, 0.4480, 0.5526, 0.5515,\n",
      "        0.5525, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5525, 0.5525, 0.5526, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5440, 0.5525, 0.5525, 0.5526, 0.5494, 0.5526, 0.4479, 0.5525, 0.5515,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5526, 0.4392, 0.4371, 0.5495, 0.5496, 0.5526,\n",
      "        0.5439, 0.5525, 0.5526, 0.5526, 0.5495, 0.5526, 0.4479, 0.5526, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5526, 0.5526, 0.4392, 0.4371, 0.5495, 0.5496, 0.5526,\n",
      "        0.5438, 0.5525, 0.5525, 0.5526, 0.5494, 0.5526, 0.4478, 0.5525, 0.5515,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5526, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5525, 0.5525, 0.5526, 0.5494, 0.5526, 0.4478, 0.5526, 0.5515,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5525, 0.5526, 0.5526, 0.5494, 0.5527, 0.4477, 0.5525, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5526, 0.5526, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5439, 0.5525, 0.5525, 0.5526, 0.5494, 0.5526, 0.4477, 0.5525, 0.5516,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5496, 0.5526,\n",
      "        0.5439, 0.5525, 0.5526, 0.5526, 0.5494, 0.5526, 0.4476, 0.5525, 0.5516,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5525, 0.5526, 0.5526, 0.5495, 0.5526, 0.4476, 0.5525, 0.5516,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5526, 0.5526, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5439, 0.5525, 0.5525, 0.5526, 0.5494, 0.5526, 0.4475, 0.5526, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5526, 0.5526, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5440, 0.5525, 0.5526, 0.5526, 0.5494, 0.5526, 0.4475, 0.5525, 0.5516,\n",
      "        0.5526, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5526, 0.5525, 0.4392, 0.4371, 0.5495, 0.5496, 0.5525,\n",
      "        0.5440, 0.5525, 0.5526, 0.5526, 0.5495, 0.5527, 0.4474, 0.5526, 0.5515,\n",
      "        0.5525, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5526, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5525, 0.5526, 0.5526, 0.5495, 0.5526, 0.4474, 0.5526, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5526,\n",
      "        0.5439, 0.5525, 0.5526, 0.5526, 0.5494, 0.5527, 0.4474, 0.5525, 0.5516,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5525, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5439, 0.5525, 0.5526, 0.5526, 0.5495, 0.5527, 0.4473, 0.5526, 0.5516,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5525, 0.5525, 0.5526, 0.4392, 0.4371, 0.5495, 0.5496, 0.5526,\n",
      "        0.5439, 0.5525, 0.5525, 0.5526, 0.5495, 0.5526, 0.4473, 0.5525, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5525, 0.5525, 0.5525, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5440, 0.5525, 0.5525, 0.5526, 0.5495, 0.5527, 0.4472, 0.5526, 0.5515,\n",
      "        0.5526, 0.5526], grad_fn=<SelectBackward>)\n",
      "torch.Size([20, 20])\n",
      "tensor([0.5526, 0.5525, 0.5526, 0.5526, 0.4392, 0.4371, 0.5495, 0.5495, 0.5525,\n",
      "        0.5439, 0.5525, 0.5525, 0.5526, 0.5494, 0.5526, 0.4472, 0.5526, 0.5515,\n",
      "        0.5525, 0.5525], grad_fn=<SelectBackward>)\n",
      "torch.Size([7, 20])\n",
      "Results: test loss:  0.6918485079492841\n",
      "Fake News accuracy:  53.196527229676406 %\n"
     ]
    }
   ],
   "source": [
    "#tester results\n",
    "correct_outputs = 0\n",
    "model.load_state_dict(torch.load('./bestmodelyet.pt'))\n",
    "model_hidden = model.hidden_initialize(batch_size)\n",
    "\n",
    "test_run_losses = []\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    print(inputs.shape)\n",
    "    if(inputs.shape[0] == 20):\n",
    "        \n",
    "        model_hidden = tuple([each.data for each in model_hidden])\n",
    "        inputs=inputs.to(device)\n",
    "        labels =labels.to(device)\n",
    "        res=model(inputs, model_hidden)\n",
    "        output= res[0]\n",
    "        model_hidden=res[1]\n",
    "        print(output)\n",
    "        go_res=output.squeeze()\n",
    "        test_loss = criteria(go_res, labels.float())\n",
    "        test_run_losses.append(test_loss.item())\n",
    "\n",
    "        results = torch.round(go_res) \n",
    "        correct_tensor = results.eq(labels.float().view_as(results))\n",
    "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "        correct_outputs= correct_outputs+ np.sum(correct)\n",
    "test_loss_mean=np.mean(test_run_losses)\n",
    "\n",
    "print(\"Results: test loss: \",test_loss_mean)\n",
    "correct_percentage = correct_outputs/len(test_loader.dataset)\n",
    "print(\"Fake News accuracy: \",100* correct_percentage,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'min_valid_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-89c14b86147e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_valid_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loss_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'min_valid_loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(min_valid_loss)\n",
    "print(valid_loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1267"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
