{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "molecular-security",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in c:\\users\\navyasree\\anaconda3\\envs\\fakenews\\lib\\site-packages (0.14.9)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in c:\\users\\navyasree\\anaconda3\\envs\\fakenews\\lib\\site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: py-params>=0.9.6 in c:\\users\\navyasree\\anaconda3\\envs\\fakenews\\lib\\site-packages (from bert-for-tf2) (0.10.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\navyasree\\anaconda3\\envs\\fakenews\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\navyasree\\anaconda3\\envs\\fakenews\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.59.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\navyasree\\anaconda3\\envs\\fakenews\\lib\\site-packages (0.1.95)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defined-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "included-gauge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bibliographic-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cross-nerve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12791, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all = pd.read_csv(\"all.tsv\", delimiter = \"\\t\")\n",
    "train_all.isnull().values.any()\n",
    "train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "usual-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cellular-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crude-working",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12791\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "labels = []\n",
    "\n",
    "with open(\"all.tsv\", 'r', encoding = 'utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        labels.append(row[2])\n",
    "        if(len(row)>13):\n",
    "            article = row[2]+row[3]+row[4]+row[5]+row[7]+row[13]\n",
    "        else:\n",
    "            article = row[1]\n",
    "        articles.append(preprocess_text(article))\n",
    "\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "domestic-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_articles(txt):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "international-volunteer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12791\n"
     ]
    }
   ],
   "source": [
    "tokenized_articles = [tokenize_articles(article) for article in articles]\n",
    "print(len(tokenized_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sapphire-hours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             false\n",
      "1          halfTrue\n",
      "2        mostlyTrue\n",
      "3             false\n",
      "4          halfTrue\n",
      "            ...    \n",
      "12786      halfTrue\n",
      "12787    barelyTrue\n",
      "12788    barelyTrue\n",
      "12789    barelyTrue\n",
      "12790         false\n",
      "Name: label, Length: 12791, dtype: object\n",
      "[4 1 2 ... 3 3 4]\n"
     ]
    }
   ],
   "source": [
    "y = train_all['label']\n",
    "print(y)\n",
    "label_val = ['true', 'halfTrue', 'mostlyTrue', 'barelyTrue', 'false', 'pantsFire']\n",
    "#label_val = ['real', 'fake']\n",
    "label_token = [0, 1, 2, 3, 4, 5]\n",
    "y = np.array(list(map(lambda x: label_val.index(x), y)))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conceptual-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_with_len = [[tokenized_articles[i], y[i], len(tokenized_articles[i])]\n",
    "                 for i in range(0, len(articles))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "virgin-aerospace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6270], 4, 1]\n"
     ]
    }
   ],
   "source": [
    "input_with_len.sort(key=lambda x: x[2])\n",
    "print(input_with_len[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "horizontal-booth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12791\n",
      "([6270], 4)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sorted_articles_labels = [(article_lab[0], article_lab[1]) for article_lab in input_with_len]\n",
    "print(len(sorted_articles_labels))\n",
    "print(sorted_articles_labels[0])\n",
    "random.shuffle(sorted_articles_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "experimental-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_articles_labels, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "useful-peoples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(512, 469), dtype=int32, numpy=\n",
       " array([[ 2026,  2767,  2057, ...,     0,     0,     0],\n",
       "        [19369,  2317, 15666, ...,     0,     0,     0],\n",
       "        [ 1037,  3660, 12211, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 2758,  2039,  2000, ...,     0,     0,     0],\n",
       "        [ 2758,  1999,  4419, ...,     0,     0,     0],\n",
       "        [ 2076,  1996,  3841, ...,     0,     0,     0]])>,\n",
       " <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
       " array([2, 1, 5, 0, 1, 1, 0, 2, 3, 3, 3, 4, 4, 2, 4, 0, 3, 1, 0, 0, 0, 2,\n",
       "        2, 1, 0, 1, 4, 4, 1, 1, 3, 4, 4, 0, 3, 4, 1, 1, 1, 5, 2, 4, 2, 2,\n",
       "        3, 0, 3, 4, 1, 0, 1, 5, 1, 1, 4, 5, 2, 1, 1, 1, 4, 3, 4, 5, 0, 3,\n",
       "        3, 0, 3, 4, 5, 5, 3, 4, 1, 0, 4, 4, 3, 0, 0, 4, 2, 1, 4, 5, 0, 0,\n",
       "        0, 4, 4, 1, 4, 3, 0, 4, 2, 2, 2, 4, 2, 3, 1, 3, 5, 4, 4, 2, 0, 1,\n",
       "        0, 2, 1, 1, 1, 1, 4, 3, 4, 4, 4, 1, 0, 1, 5, 4, 1, 3, 3, 5, 1, 2,\n",
       "        0, 1, 4, 4, 0, 2, 0, 1, 1, 2, 2, 3, 2, 3, 5, 1, 5, 0, 2, 3, 5, 3,\n",
       "        1, 1, 5, 5, 4, 2, 3, 5, 1, 0, 2, 2, 4, 1, 1, 1, 3, 1, 4, 1, 5, 3,\n",
       "        0, 1, 4, 0, 4, 2, 4, 5, 5, 4, 2, 0, 1, 4, 4, 0, 2, 1, 4, 1, 2, 4,\n",
       "        2, 2, 2, 2, 2, 4, 4, 3, 1, 5, 4, 2, 4, 2, 1, 4, 2, 4, 2, 3, 4, 2,\n",
       "        0, 3, 2, 4, 2, 0, 2, 1, 4, 1, 4, 2, 4, 5, 3, 5, 0, 0, 0, 4, 0, 1,\n",
       "        3, 5, 3, 5, 4, 3, 4, 3, 2, 4, 5, 0, 3, 3, 5, 1, 4, 0, 2, 1, 2, 1,\n",
       "        2, 1, 2, 1, 4, 1, 4, 2, 2, 5, 3, 3, 2, 0, 3, 5, 4, 0, 3, 5, 1, 4,\n",
       "        4, 5, 2, 1, 3, 0, 4, 4, 0, 3, 2, 4, 4, 1, 3, 3, 3, 5, 0, 2, 4, 3,\n",
       "        4, 3, 3, 4, 5, 3, 0, 2, 1, 2, 4, 5, 4, 5, 5, 3, 1, 2, 2, 3, 4, 4,\n",
       "        4, 3, 2, 5, 3, 4, 0, 2, 2, 2, 1, 3, 1, 2, 2, 0, 2, 4, 0, 4, 2, 3,\n",
       "        4, 2, 3, 2, 0, 4, 2, 5, 0, 2, 1, 2, 2, 1, 1, 1, 3, 4, 0, 0, 2, 5,\n",
       "        5, 2, 3, 1, 1, 0, 4, 4, 2, 5, 0, 1, 1, 4, 1, 3, 4, 4, 4, 0, 0, 4,\n",
       "        0, 2, 5, 1, 4, 3, 3, 4, 4, 3, 3, 0, 4, 1, 5, 0, 5, 4, 4, 1, 4, 1,\n",
       "        2, 0, 3, 2, 2, 5, 5, 2, 3, 3, 2, 4, 4, 1, 2, 3, 1, 1, 1, 4, 2, 0,\n",
       "        2, 4, 5, 3, 0, 2, 2, 3, 3, 0, 1, 1, 4, 3, 5, 1, 1, 4, 4, 3, 5, 3,\n",
       "        2, 2, 4, 5, 3, 3, 3, 1, 0, 4, 3, 1, 2, 2, 0, 3, 1, 3, 1, 5, 3, 1,\n",
       "        5, 4, 4, 2, 2, 4, 4, 2, 1, 2, 3, 1, 5, 4, 3, 2, 2, 3, 1, 1, 0, 0,\n",
       "        2, 3, 4, 2, 1, 3])>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n",
    "next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "scientific-gasoline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "TOTAL_BATCHES = math.ceil(len(sorted_articles_labels) / BATCH_SIZE)\n",
    "print(TOTAL_BATCHES)\n",
    "TEST_BATCHES = TOTAL_BATCHES // 20\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "opposed-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Bidirectional(LSTM(embedding_dimensions))\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "confident-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 6\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "wireless-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unusual-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"accuracy\"])\n",
    "else:\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "previous-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "24/24 [==============================] - 244s 10s/step - loss: 1.7633 - sparse_categorical_accuracy: 0.1914\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 197s 8s/step - loss: 1.6881 - sparse_categorical_accuracy: 0.2840\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 223s 10s/step - loss: 1.5618 - sparse_categorical_accuracy: 0.3661\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 255s 11s/step - loss: 1.3575 - sparse_categorical_accuracy: 0.4830\n",
      "Epoch 5/5\n",
      "24/24 [==============================] - 245s 10s/step - loss: 1.0918 - sparse_categorical_accuracy: 0.6149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f0cc12470>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "appreciated-african",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 9s 9s/step - loss: 2.2362 - sparse_categorical_accuracy: 0.2344\n",
      "[2.2361536026000977, 0.234375]\n"
     ]
    }
   ],
   "source": [
    "results = text_model.evaluate(test_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "residential-inspector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  6104400   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              multiple                  40100     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            multiple                  60100     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional multiple                  641600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  153856    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  1542      \n",
      "=================================================================\n",
      "Total params: 7,001,598\n",
      "Trainable params: 7,001,598\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-amendment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
